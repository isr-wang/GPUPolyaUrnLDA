\documentclass{article}

\usepackage{color}
\usepackage{parskip}
\usepackage{amsmath}


\let\textv\v %redefinition: v under letter in text
\renewcommand{\v}{\relax\ifmmode\expandafter\boldsymbol\else\expandafter\textv\fi} %vector
\newcommand{\m}{\expandafter\mathbf} %matrix
\let\goesto\rightarrow

\newcommand{\mm}[1]{{\textcolor{magenta}{[Mans: #1]}}}
\newcommand{\at}[1]{{\textcolor{green}{[Alex: #1]}}}
\newcommand{\ks}[1]{{\textcolor{blue}{[Kunal: #1]}}}

\title{Design - GPU implementation of Polya-Urn LDA}
\author{Alex Terenin, M{\aa}ns Magnusson, Kunal Sarkhel, David Draper}

\begin{document}

\maketitle


\section{Introduction - purpose of document}

This document contains the design considerations of the GPU Polya-Urn LDA sampler together with implementations. The purpose is to describe challenges and potential solutions for this implementation in a way that makes it easy to convert this document into an article later on.

\subsection{Notation}

\begin{center}
\small{
\begin{tabular}{c l c c l}
\hline
Symbol & Description && Symbol & Description
\\
\cline{1-2} \cline{4-5}
$V$ & Vocabulary size && $\m{\Phi} : K \times V$ & Word-topic probabilities
\\
$D$ & Total number of documents && $\phi_k : 1 \times V$ & Word probabilities for topic $k$
\\
$N$ & Total number of words && $\beta : 1 \times V$ & Prior concentration vector for $\phi_k$
\\
$K$ & Total number of topics && $\m{n} : K \times V$ & Topic-word sufficient statistic
\\
$v(i)$ & Word type for word $i$ && $\m{\Theta} : D \times K$ & Document-topic probabilities
\\
$d(i)$ & Document for word $i$ && $\theta_d : 1 \times K$ & Topic probabilities for document $d$
\\
$w_{i,d}$ & Word $i$ in document $d$ && $\alpha : 1 \times K$ & Prior concentration vector for $\theta_d$
\\
$z_{i,d}$ & Topic indicator for word $i$ in $d$ && $\m{m} : D \times K$ & Document-topic sufficient statistic
\\
\hline
\end{tabular}
}
\end{center}

\subsection{Limitations}

\begin{itemize}
\item 1M topics
\item 150K vocabulary
\item 1K common words
\item 10K max topics per document
\item 1K max documents per buffer
\item No limit on number of documents
\end{itemize}

\section{Steps of Algorithm}

\begin{itemize}

\item Sample $z_{i,d} \propto \phi_{k,v(i)} \alpha_k + \phi_{k,v(i)} + m_{d,k}^{-i}$

\begin{itemize}
\item Load $\v{z}$ from disk
\item Sample $\v{z}$
\end{itemize}

\item Sample $\v\phi_k \sim \text{Poisson-P\'{o}lyaUrn} (\v{n}_k + \v\beta)$

\begin{itemize}
\item Sample $\tilde\gamma_{k,v} \sim \text{Pois}(\beta_v + n_{n,v})$
\item If $\tilde\gamma_{k,v} \neq 0$: add $\tilde\gamma_{k,v}$ to $\m\Phi$
\item Compute $\tilde\gamma_{k,\v\cdot} = \sum_{v=1}^V \tilde\gamma_{k,v}$ and set $\phi_{k,v} = \tilde\gamma_{k,v} / \tilde\gamma_{k,\v\cdot}$ in-place
\item Transpose $\m\Phi$ by sorting it by column
\end{itemize}
\end{itemize}

\section{Steps for implementation}

\begin{enumerate}
  \item Implement a function to convert a corpus to binary, main memory, format. The function should also write out the vocabulary and document ids.
  \item Implement SabreLDA Warp-voting
  \item Implement W-ary trees
  \item Implement SabreLDA
  \item Implement Polya-Urn LDA  
\end{enumerate}


\section{Questions, ideas and comments}

Below are some question and general comments to discuss further.

\begin{itemize}
  \item SabreLDA is storing the $\mathbf{A}$ matrix ($\mathbf{m}$) on disc as well. They also recalculate the whole vector each time? Why? Do we need to store $\mathbf{A}$? In Mallet we recalculate this on the fly when we sample a new document (called \texttt{localTopicCounts}). It should be fast to compute $m_d$ with the method they propose. They do not seem to update $\mathbf{A}$ in each step instead of doing it after a chunk of documents?
  \item They partition by document chunks and then by word order. The motivation for this is that $\hat{B}$ is dense while $\mathbf{A}$ is sparse. We do not have this situation, so I'm not sure we need to do this?
  \item I know that in the paper on large scale LDA by Yahoo LDA (Smola et al. 2012) they store different parts of the matrix $n$ in dense and sparse format depending if the words are rare or common. Rare words are stored sparsely while more common word are stored in a dense matrix. Maybe we should do something similar?
  \item We have sparsity in $\Phi$, and that makes it possible for us to produce sparse Alias tables (or sparse W-ary trees), something that they cannot do. This is a benefit for us - it will reduce the cost of sampling with W-ary trees (or sparse Alias tables) quite much.
  \item The sparsity in Phi will be quite large. We will have a smaller memory need (for Pubmed approx 1-2\% of what they need, for $\beta = 0.01$).
  \item They think updating A is difficult due to sparsity. We will have that problem but much more aggravated. We both have a sparse $m$ and $n$. 
  \item Similar to SabreLDA, we want to study the memory cost of: The $m$, $n$, $\Phi$ and the Wary-trees/Alias tables. We also want to study how much time each part takes: sampling $z$, sampling $\Phi$, I/O (streaming), and construction of the Wary-trees/sparse Alias tables.
\end{itemize}


\section{Challenges}

\subsection{Computing the intersection of two sparse vectors}

In the core of the Gibbs sampling algorithm one of the main parts are to sample from the two vectors $\phi_v \times m_d$ (element-wise multiplication and cumulative sum. Since both vectors are sparse, it can be difficult to compute this element efficiently in $O(\min(K_d, K_v)$ time.

Potential solutions are:

\begin{itemize}
  \item Skiplists \mm{Alex fill out this more, why this would be smart}
  \item Hashmaps for storing $\Phi$ (and $m$). In a hashmap may both save storage and would still make it possible to do a lookup in constant time. \mm{Question: How big is a Hashmap memory footprint? And how do these HashMaps handle multiple threads writing to the same position?}
  \item Maybe use local (non-sparse) arrays for $m$ since this is updated in each iteration?
\end{itemize}

\subsection{Alias tables or sparse W-ary trees}

In SabreLDA they conclude that the construction of an Alias table construction is $O(K)$ and this table can be used to sample in $O(1)$ time. But this, according to the authors, cannot be done on a GPU, since implementing the Alias table is sequential. 

The original Walker-Alias algorithm is sequential, but it should be able to do in a GPU setting, although it may not be as efficient as constructing a W-ary tree. But constructing the (sparse) Alias tables are still just a tiny part of the algorithm while drawing samples is used extensively. I think we should study using the Alias approach.

The construction of a Walker Alias table is not fully sequential although it is described this way. 

I'm thinking of a potential solution where we first sort the probabilities. This would, if we have a sparse $\Phi$, eventually be possible to create a table in $O(K_v \cdot \log(K_v)$ by sorting the proportions before creating the table and sorting is something that can be done effectively in parallel. I need to sketch a little bit on this. But if we would be able to create an Alias table we would get a better complexity. 

The Wary Trees, on the other hand, would need to be applied to a sparse situation. I do not think this should be that difficult, and then the wary trees can probably be extremely fast. But they need to be adapted to a sparse vector.

\subsection{Updating $m$ and $n$}

Unlike SableLDA we need to update $m$ in sampling each token $z_i$ in each iteration \mm{I do not know why they just ignore this in their approach}. If $m_d$ is stored in sparse format updating them will probably be relatively costly as well. In each iteration we need to decrease $m_{d,kold}$ and increase $m_{d,knew}$. Updating $n$ on, the other hand, can be done chunk-wise after all sampling are done for a document, chunk of documents or the whole corpus.

\section{Formats}

\subsection{I/O formats}

The basic format of the textual documents should be the same format as is used by the Mallet framework. This means that each document is stored as one document per line with tokens separated by space. The first element can be a document id (then it is separated by \texttt{\\tab}).

The second format is the binary format written to disc that consists of both the topic indicator and the tokens. When we read in the data and transforms it to binary format we need to write out three files:

\begin{itemize}
  \item A binary token-topic indicator file.
  \item The vocabulary (one row per word type)
  \item Potentially document ids (one row per doc-id)
\end{itemize}

Since we want to stream in the binary indicator file from disc, we would probably need to minimize the memory footprint for this file. There are different ways we can store this, but all consists of storing one document per row. One solution would be that every token/topic pair is stored as:

\bigskip

\texttt{type\_idx1/topic\_idx1 type\_idx2/topic\_idx2 ... type\_idxNd/topic\_idxNd}

\bigskip

This should be a sufficiently memory efficient binary format.

\begin{itemize}
\item We have committed to the Mallet input format.
\item Serialization: data written directly to binary format
\item Random Number Generation: use \emph{Philox} via the device API
\item Output to Mallet state file
\end{itemize}

\subsection{Internal representation}

In the GPU implementation of SabreLDA, they use the pair $L = [d, v, k]$ to represent a token. But this means that $d$ is stored redundantly. I do not know if it would be more efficient to store it as two arrays per document $D = [[v_1, ..., v_{N_d}], [z_1, ..., z_{N_d}]]$. That representation is more memory efficient in a CPU framework, but I do not know the potential cache effects.

\subsection{What gets stored where}
\begin{itemize}
\item $\m{z}$: streaming from disk, structure consisting of document ID, length of document, array of tokens, array of topic indicators
\item $\m\Phi$: stored on GPU, in some sparse format -- initially a lossy hash table
\item Alias tables for $\m\Phi$: stored on GPU, recomputed when $\m\Phi$ is sampled.
\item Alias tables for Poisson RVs: precomputed at start
\item $\m{n}$: stored on GPU in some sparse format -- initially a lossy hash table
\end{itemize}

\subsection{Binary Format for $\v{z}$}

Need to store the 3-tuple $(d, w_{i,d}, z_{i,d})$. This will be stored as arrays in binary format on disk and in a preallocated buffer in memory.
\begin{itemize}

\item $\v{w}$: token id
\begin{itemize}
\item unsigned int array
\item 4 byte per token
\end{itemize}

\item $\v{z}$: topic indicator
\begin{itemize}
\item unsigned int array
\item 4 byte per token
\item token id sorted by rarity: most common words first
\end{itemize}

\item $\v{d}$: document length
\begin{itemize}
\item unsigned int array
\item 4 byte per document
\end{itemize}

\end{itemize}

\subsection{Binary Format for $\m\Phi$}

Rows of $\m\Phi$ correspond to topics. 
Columns of $\m\Phi$ correspond to unique words.
$\m\Phi$ is stored in floating point.
$\m\Phi$ is sampled in two steps by (1) drawing Poisson RVs elementwise, and (2) normalizing the rows.
$\m\Phi$ is accessed column-wise by the warp sampler.

Problem: given hash collisions, how to handle normalization? One approach: store the normalizing constant for each row of $\m\Phi$, and access it when loading nonzero elements of $\m\Phi$.

Problem: we want to load the necessary column of $\m\Phi$ into shared memory. But we can't load the entire hash table.

Problem: how to ensure roundoff error is not too bad? Floating point arithmetic is about 3x faster on GPUs, so we want to use it as much as possible.

\subsection{Binary Format for $\m{n}$}

Very similar to $\m\Phi$. 
However, $\m{n}$ is stored as integers rather than floats.
For every pass over the data, $\m{n}$ is \emph{calculated from scratch}.
This is because (1) adding the current value is cheaper than subtracting the old one and adding the new one, especially in sparse format (2) because it means we don't need to initialize $\m{n}$, and (3) this makes a distributed implementation easier, as multiple systems can perform a distributed add.
Thus, after sampling $\m\Phi$, we reset $\m{n}$ to a matrix of zeros, and accumulate its values atomically in the warp sampler.

\subsection{Binary Format for $\v\sigma_a$}

Array of floats of size $V$.

\subsection{Binary Format for the Sparse Alias tables}

We must store an Alias table for each unique word.
Since the Alias tables are sparse, we cannot use the index of each cell as the value being sampled.
Thus, we store the size of the table, and three arrays, as follows.
\begin{itemize}
\item[--] $\v{p}^{\v{A}^{(1)}}$: probability of landing in first Alias array.
\item[--] $\v{A}^{(1)}$: first Alias array.
\item[--] $\v{A}^{(2)}$: second Alias array.
\end{itemize}

Sampling is done by (1) uniformly selecting a cell $i$ in $\v{p}^{\v{A}^{(1)}}$, (2) returning $A^{(1)}_i$ with probability $p^{\v{A}^{(1)}}_i$ and $A^{(2)}_i$ otherwise. This requires 1 read of 3 values from global memory.

Building the Alias table is done as follows. We access each column of $\m\Phi$ in parallel using multiple warps. Each warp performs a warp shuffle and determines how many non-zero elements it found. It atomically increments this in shared memory, and then writes its non-zero elements to shared memory, using a warp vote to figure out its place in the array. From here, the problem is reduced to building a dense Alias table in parallel.

Problem: the size of each table is variable and will change per iteration. How much memory to allocate for each table? Should we load $\v{V}$ and use a heuristic?

\subsection{Binary Format for $\m{m}$}

This is the document-topic sufficient statistic, which is independent across documents, and is calculated on the fly in the warp sampling kernel kernel in shared memory. 

Problem: we need to store it sparsely with $O(1)$ access, since a dense array won't fit in shared memory. How to do this?

Problem: we need to update $\m{m}$ after sampling each token.

\subsection{Binary Format for the Poisson Alias tables}

Stored as a dense Alias table, i.e. two arrays.

\subsection{Binary Format for $\v{V}$}

We need to store $\v{V}$ for three reasons: (1) in the preprocess step, since we use PDOW format, we need the mapping token (string) $\goesto$ token id (int), (2) in the output step, we need to print in a user-legible format, for which we need the mapping token id (int) $\goesto$ token (string), and (3) in training, we need to know the number of unique words.

\section{Parallel Alias table construction}

DONE. I have designed and implemented a synchronization-free non-divergent extension of Vose's method.

\section{Hash Map}

Where HashMap is used:

\begin{enumerate}
\item build m: 				accumulate<warp>(1)
\item remove from m			accumulate-no-insert<thread>(-1)
\item compute m*Phi			get<warp>
\item add to m				accumulate<thread>(1)
\item add to n (hashmap for every topic)	accumulate<???>(1)
\item sample Phi rows: nonzero		---
\item sample Phi rows: zero		insert<warp>
\item normalize Phi			---
\item transpose Phi			insert<block>
\end{enumerate}

\subsection{Lockfree Robin Hood Hashing}

Robin Hood guarantees for fast lookup
\begin{itemize}
\item Stop when we hit an empty slot
\item Stop when we hit a slot that's closer than the one we want
\end{itemize}

Insert
\begin{itemize}
\item Forward pass: compute Robin Hood path
\begin{itemize}
\item Start with the item's hash bucket, set counter to 0, set new to 1, and repeat until exit.
\item Search linearly, incrementing counter, until we find either an empty slot, or a slot whose distance is less than than the counter
\item If the slot is empty: place the item into that slot, and exit
\item Else if slot is locked: wait until unlock.
\item If new is 1: store the counter value in the backpointer, with a flag indicating that it is new, and set new to 0
\item Else if new is 0: store a backpointer to the previous element in its slot
\end{itemize}
\item Backward pass: place item
\begin{itemize}
\item Start with empty slot, adding a backpointer to indicate it is being moved
\item Atomically swap it with the slot pointed to by its backpointer
\item Once we hit the new item's slot, if is stealable, place it into the slot
\item If item is not stealable and does not have a backpointer, restart insertion
\end{itemize}
\end{itemize}

Modification
\begin{itemize}
\item If item is present, atomically increment
\item If item is not present, insert it
\end{itemize}

Deletion
\begin{itemize}
\item Add a tombstone element
\end{itemize}

Guarantees
\begin{itemize}
\item Items are always in the table, but might be in the table twice
\end{itemize}

\subsection{Collaborative Robin Hood Hashing}

Layout
\begin{itemize}
\item 1 bit: relocation flag
\item 3 bit: hash backpointer \{none, buffer, $h^*_0$, $h_1$, $h_2$, $h_3$, $h_4$, $h_5$\}
\item 4 bits: index backpointer \{$0,..,15$\}
\item 20 bits: key
\item 36 bits: value
\end{itemize}

Get2

Accumulate2


\end{document}
